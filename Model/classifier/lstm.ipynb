{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader , TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from lstm_net import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(715, 301, 6) (230, 301, 6) (235, 301, 6)\n",
      "(715,) (230,) (235,)\n"
     ]
    }
   ],
   "source": [
    "names = ['AC', 'AD', 'BC', 'BD']\n",
    "types = ['train', 'val', 'test']\n",
    "#load data\n",
    "'''AC = np.load('C:\\Github_LIU\\SII2025\\process_data\\classifier\\AC.npy')\n",
    "AD = np.load('C:\\Github_LIU\\SII2025\\process_data\\classifier\\AD.npy')\n",
    "BC = np.load('C:\\Github_LIU\\SII2025\\process_data\\classifier\\BC.npy')\n",
    "BD = np.load('C:\\Github_LIU\\SII2025\\process_data\\classifier\\BD.npy')'''\n",
    "for name in names:\n",
    "    for type in types:\n",
    "        globals()[name + '_' + type] = np.load('/Users/syunsei/Desktop/SII2025/process_data/classifier/' + name + '_' + type + '.npy')\n",
    "        \n",
    "X_train = np.concatenate((AC_train, AD_train, BC_train, BD_train), axis=0)\n",
    "X_val = np.concatenate((AC_val, AD_val, BC_val, BD_val), axis=0)\n",
    "X_test = np.concatenate((AC_test, AD_test, BC_test, BD_test), axis=0)\n",
    "y_train = np.concatenate((np.zeros(AC_train.shape[0]), np.ones(AD_train.shape[0]), np.ones(BC_train.shape[0]) * 2, np.ones(BD_train.shape[0]) * 3), axis=0)\n",
    "y_val = np.concatenate((np.zeros(AC_val.shape[0]), np.ones(AD_val.shape[0]), np.ones(BC_val.shape[0]) * 2, np.ones(BD_val.shape[0]) * 3), axis=0)\n",
    "y_test = np.concatenate((np.zeros(AC_test.shape[0]), np.ones(AD_test.shape[0]), np.ones(BC_test.shape[0]) * 2, np.ones(BD_test.shape[0]) * 3), axis=0)\n",
    "\n",
    "#split data into training and testing and validation\n",
    "X_train=np.concatenate ((X_train[:,:,0:3],X_train[:,:,6:9]), axis=2)\n",
    "X_val=np.concatenate ((X_val[:,:,0:3],X_val[:,:,6:9]), axis=2)\n",
    "X_test=np.concatenate ((X_test[:,:,0:3],X_test[:,:,6:9]), axis=2)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)\n",
    "'''\n",
    "#归一化\n",
    "def min_max_normalize(data):\n",
    "    min_val = np.min(data, axis=0)\n",
    "    max_val = np.max(data, axis=0)\n",
    "    range_val = max_val - min_val\n",
    "    range_val[range_val == 0] = 1  # 防止除零错误\n",
    "    return (data - min_val) / range_val\n",
    "#X = min_max_normalize(X)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42)\n",
    "print(X.shape,X_train.shape, X_test.shape, X_val.shape,y_train.shape)\n",
    "#draw X_train\n",
    "plt.plot(X[1110])\n",
    "plt.show()'''\n",
    "\n",
    "batch_size = 100\n",
    "train_data = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_data = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val))\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_data = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test))\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "sequence_len = 301\n",
    "input_len = 6\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 4\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(6, 128, num_layers=2, batch_first=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(input_len, hidden_size, num_layers, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m     28\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m plot_learning_curve(loss_list)\n",
      "Cell \u001b[0;32mIn[50], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, train_loader, loss_function)\u001b[0m\n\u001b[1;32m      6\u001b[0m lifts \u001b[38;5;241m=\u001b[39m lifts\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_len, input_len)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlifts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/SII2025/Model/classifier/lstm_net.py:14\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     15\u001b[0m     cell_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, (hidden_state, cell_state))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "def train(num_epochs, model, train_loader, loss_function):\n",
    "    total_step = len(train_loader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (lifts, labels) in enumerate(train_loader):\n",
    "            lifts = lifts.reshape(-1, sequence_len, input_len).to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            \n",
    "            outputs = model(lifts)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch+1) % 2 == 0:\n",
    "                loss_list.append(loss.item())\n",
    "                print(f\"Epoch [{epoch+1}; Batch {batch+1}/{total_step}]; Loss: {loss.item():.4f}\")\n",
    "                \n",
    "loss_list = []\n",
    "#draw learning curve\n",
    "def plot_learning_curve(loss_list):\n",
    "    plt.plot(loss_list, label=\"loss\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "train(num_epochs, model, train_loader, loss_function)\n",
    "plot_learning_curve(loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 98.13084112149532%\n"
     ]
    }
   ],
   "source": [
    "def validate(model, val_loader):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for lifts, labels in val_loader:\n",
    "            lifts = lifts.reshape(-1, sequence_len, input_len).to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            \n",
    "            outputs = model(lifts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        print(f\"Validation accuracy: {100 * correct / total}%\")\n",
    "\n",
    "validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.30508474576271%\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for lifts, labels in test_loader:\n",
    "            lifts = lifts.reshape(-1, sequence_len, input_len).to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            \n",
    "            outputs = model(lifts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        print(f\"Test accuracy: {100 * correct / total}%\")\n",
    "\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
