{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader , TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from lstm_cnn_net import lstm_cnn\n",
    "from lstm_net import LSTM\n",
    "from cnn_net import MyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([715, 301, 12]) torch.Size([46, 301, 12]) torch.Size([47, 301, 12])\n",
      "torch.Size([715]) torch.Size([46]) torch.Size([47])\n"
     ]
    }
   ],
   "source": [
    "names = ['AC', 'AD', 'BC', 'BD']\n",
    "types = ['train', 'val', 'test']\n",
    "#load data\n",
    "#mac: /Users/syunsei/Desktop/SII2025/process_data/classifier/\n",
    "#win: C:\\Github_LIU\\SII2025\\process_data\\classifier\\\n",
    "for name in names:\n",
    "    for type in types:\n",
    "        globals()[name + '_' + type] = np.load('/Users/syunsei/Desktop/SII2025/process_data/classifier/' + name + '_' + type + '.npy')\n",
    "        \n",
    "X_train = np.concatenate((AC_train, AD_train, BC_train, BD_train), axis=0)\n",
    "X_val = np.concatenate((AC_val, AD_val, BC_val, BD_val), axis=0)\n",
    "X_test = np.concatenate((AC_test, AD_test, BC_test, BD_test), axis=0)\n",
    "y_train = np.concatenate((np.zeros(AC_train.shape[0]), np.ones(AD_train.shape[0]), np.ones(BC_train.shape[0]) * 2, np.ones(BD_train.shape[0]) * 3), axis=0)\n",
    "y_val = np.concatenate((np.zeros(AC_val.shape[0]), np.ones(AD_val.shape[0]), np.ones(BC_val.shape[0]) * 2, np.ones(BD_val.shape[0]) * 3), axis=0)\n",
    "y_test = np.concatenate((np.zeros(AC_test.shape[0]), np.ones(AD_test.shape[0]), np.ones(BC_test.shape[0]) * 2, np.ones(BD_test.shape[0]) * 3), axis=0)\n",
    "\n",
    "#归一化\n",
    "def min_max_normalize(data):\n",
    "    min_val = np.min(data, axis=0)\n",
    "    max_val = np.max(data, axis=0)\n",
    "    range_val = max_val - min_val\n",
    "    range_val[range_val == 0] = 1  # 防止除零错误\n",
    "    return (data - min_val) / range_val\n",
    "#X_train = min_max_normalize(X_train)\n",
    "#X_val = min_max_normalize(X_val)\n",
    "#X_test = min_max_normalize(X_test)\n",
    "\n",
    "#split data into training and testing and validation\n",
    "'''X_train=np.concatenate ((X_train[:,:,0:3],X_train[:,:,6:9]), axis=2)\n",
    "X_val=np.concatenate ((X_val[:,:,0:3],X_val[:,:,6:9]), axis=2)\n",
    "X_test=np.concatenate ((X_test[:,:,0:3],X_test[:,:,6:9]), axis=2)'''\n",
    "X_train=torch.tensor(X_train,dtype=torch.float32)\n",
    "X_val=torch.tensor(X_val,dtype=torch.float32)\n",
    "X_test=torch.tensor(X_test,dtype=torch.float32)\n",
    "y_train=torch.tensor(y_train,dtype=torch.long)\n",
    "y_val=torch.tensor(y_val,dtype=torch.long)\n",
    "y_test=torch.tensor(y_test,dtype=torch.long)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)\n",
    "\n",
    "\n",
    "'''#draw X_train\n",
    "plt.plot(X[1110])\n",
    "plt.show()\n",
    "\n",
    "batch_size = 100\n",
    "train_data = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_data = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val))\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_data = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test))\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)'''\n",
    "\n",
    "#dataset\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=10)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=20)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "sequence_len = 301\n",
    "input_len = 12\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 4\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(12, 128, num_layers=2, batch_first=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(input_len, hidden_size, num_layers, num_classes).to(device)\n",
    "#model = MyNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1; Batch 2/72]; Loss: 1.4813\n",
      "Epoch [1; Batch 4/72]; Loss: 1.3918\n",
      "Epoch [1; Batch 6/72]; Loss: 1.3675\n",
      "Epoch [1; Batch 8/72]; Loss: 1.3622\n",
      "Epoch [1; Batch 10/72]; Loss: 1.3024\n",
      "Epoch [1; Batch 12/72]; Loss: 1.3601\n",
      "Epoch [1; Batch 14/72]; Loss: 1.3290\n",
      "Epoch [1; Batch 16/72]; Loss: 1.2685\n",
      "Epoch [1; Batch 18/72]; Loss: 1.3055\n",
      "Epoch [1; Batch 20/72]; Loss: 1.2742\n",
      "Epoch [1; Batch 22/72]; Loss: 1.4372\n",
      "Epoch [1; Batch 24/72]; Loss: 1.2883\n",
      "Epoch [1; Batch 26/72]; Loss: 1.4463\n",
      "Epoch [1; Batch 28/72]; Loss: 1.4259\n",
      "Epoch [1; Batch 30/72]; Loss: 1.3259\n",
      "Epoch [1; Batch 32/72]; Loss: 1.3578\n",
      "Epoch [1; Batch 34/72]; Loss: 1.3808\n",
      "Epoch [1; Batch 36/72]; Loss: 1.2707\n",
      "Epoch [1; Batch 38/72]; Loss: 1.3970\n",
      "Epoch [1; Batch 40/72]; Loss: 1.2934\n",
      "Epoch [1; Batch 42/72]; Loss: 1.2409\n",
      "Epoch [1; Batch 44/72]; Loss: 1.2439\n",
      "Epoch [1; Batch 46/72]; Loss: 1.2070\n",
      "Epoch [1; Batch 48/72]; Loss: 1.2634\n",
      "Epoch [1; Batch 50/72]; Loss: 1.3166\n",
      "Epoch [1; Batch 52/72]; Loss: 1.1371\n",
      "Epoch [1; Batch 54/72]; Loss: 1.0146\n",
      "Epoch [1; Batch 56/72]; Loss: 1.0933\n",
      "Epoch [1; Batch 58/72]; Loss: 0.8712\n",
      "Epoch [1; Batch 60/72]; Loss: 1.1391\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m     29\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m plot_learning_curve(loss_list)\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, train_loader, loss_function)\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(num_epochs, model, train_loader, loss_function):\n",
    "    total_step = len(train_loader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (lifts, labels) in enumerate(train_loader):\n",
    "            for i in range(lifts.size(0)):\n",
    "                lift = lifts[i]\n",
    "                '''lifts = lifts.reshape(-1, sequence_len, input_len).to(device)\n",
    "                labels = labels.long().to(device)'''\n",
    "                lifts, labels = lift.to(device), label.to(device)\n",
    "            \n",
    "            outputs = model(lifts)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch+1) % 2 == 0:\n",
    "                loss_list.append(loss.item())\n",
    "                print(f\"Epoch [{epoch+1}; Batch {batch+1}/{total_step}]; Loss: {loss.item():.4f}\")\n",
    "                \n",
    "loss_list = []\n",
    "#draw learning curve\n",
    "def plot_learning_curve(loss_list):\n",
    "    plt.plot(loss_list, label=\"loss\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "train(num_epochs, model, train_loader, loss_function)\n",
    "plot_learning_curve(loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 78.26086956521739%\n"
     ]
    }
   ],
   "source": [
    "def validate(model, val_loader):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for lifts, labels in val_loader:\n",
    "            lifts = lifts.reshape(-1, sequence_len, input_len).to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            \n",
    "            outputs = model(lifts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        print(f\"Validation accuracy: {100 * correct / total}%\")\n",
    "\n",
    "validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 63.829787234042556%\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for lifts, labels in test_loader:\n",
    "            lifts = lifts.reshape(-1, sequence_len, input_len).to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            \n",
    "            outputs = model(lifts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        print(f\"Test accuracy: {100 * correct / total}%\")\n",
    "\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
